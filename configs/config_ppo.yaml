experiment:
  name: ppo_selfplay
  save_dir: experiments
  bc_pretrain_path: /root/shared-nvme/cjt/experiments/epoch_15_loss_1.6333.pt

model:
  obs_channels: 15
  memory_channels: 13
  base_channels: 64
  grid_size: 24

training:
  total_iterations: 10000
  n_steps_per_update: 2048 # 2048
  n_parallel_envs: 16 # 16
  batch_size: 512 # 512
  ppo_epochs: 4
  
  gamma: 0.99
  gae_lambda: 0.95
  
  clip_epsilon: 0.2
  value_clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01
  
  learning_rate: 0.0001
  max_grad_norm: 0.5
  
  warmup_iterations: 0
  eval_frequency: 25
  save_frequency: 50
  log_frequency: 5
  
  max_episode_steps: 1000
  
  device: cuda
  seed: 42

opponent_pool:
  max_size: 20
  initial_elo: 1500
  k_factor: 32
  temperature: 0.5
  update_freq: 50
  warmup_iterations: 0

reward:
  land_weight: 0.3
  army_weight: 0.3
  castle_weight: 0.4
  max_ratio: 100.0
  gamma: 0.99

multi_gpu:
  enabled: false
  strategy: ddp

logging:
  use_wandb: true
  project_name: generals-rl-ppo
  wandb_entity: null

