experiment:
  name: ppo_selfplay_ratio
  save_dir: experiments
  bc_pretrain_path: /root/shared-nvme/cjt/experiments/bc_new_mem_all_replays_20251218_233422/checkpoints/epoch_14_loss_1.8876.pt

model:
  obs_channels: 15
  memory_channels: 13
  base_channels: 64
  grid_size: 24

training:
  total_iterations: 10000
  n_steps_per_update: 2048 # 2048
  n_parallel_envs: 16 # 16
  batch_size: 512 # 512
  ppo_epochs: 4
  
  gamma: 0.99
  gae_lambda: 0.95
  
  clip_epsilon: 0.1 # Reduced from 0.2 to constrain policy updates
  value_clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.0 # Set to 0.0 for BC fine-tuning to prevent policy collapse
  
  learning_rate: 0.000001 # Reduced from 1e-5 to 1e-6 for stable fine-tuning
  max_grad_norm: 0.5
  
  lock_backbone: true # [CRITICAL] Freeze backbone to preserve BC features
  value_warmup_iterations: 5  # Freeze policy for first 10 updates to train Value Function
  warmup_iterations: 0
  eval_frequency: 25
  save_frequency: 1 # 50
  log_frequency: 1 # 25
  
  max_episode_steps: 1000
  
  device: cuda
  seed: 42

opponent_pool:
  max_size: 20
  initial_elo: 1500
  k_factor: 32
  temperature: 0.5
  update_freq: 50
  warmup_iterations: 0

reward:
  land_weight: 0.3
  army_weight: 0.8
  castle_weight: 0.1
  castle_weight: 0.3
  max_ratio: 5.0 # 100.0
  gamma: 1.00

multi_gpu:
  enabled: false
  strategy: ddp

logging:
  experiment_name: ppo_cyh
  save_dir: experiments
  use_wandb: true
  wandb_project: generals-ppo
  wandb_entity: coc  # <--- Add this line
  wandb_tags: [ppo]

