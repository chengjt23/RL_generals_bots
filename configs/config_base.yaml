training:
  batch_size: 32  # Larger batch for efficiency
  max_sequence_length: 64  # Short sequences for speed (increase gradually if needed)
  tbptt_steps: 32  # TBPTT window (standard size)
  learning_rate: 0.0003
  num_epochs: 30
  steps_per_epoch: null  # null = process all data, or set number to limit
  gradient_clip: 1.0
  warmup_steps: 1000
  weight_decay: 0.0001
  
  save_top_k: 5
  eval_every: 1000
  log_every: 100
  
  device: cuda
  num_workers: 4
  mixed_precision: true
  compile_model: false  # torch.compile can speed up but has compilation overhead

  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.0

data:
  data_dir: data/generals_io_replays
  grid_size: 24
  min_stars: 70
  max_turns: 500
  max_replays: 18803
  train_split: 0.95

model:
  obs_channels: 15
  memory_channels: 6  # Output channels from RNN memory encoder (4-8 recommended)
  base_channels: 64
  grid_size: 24
  rnn_hidden_channels: 32  # Hidden channels in RNN layers
  rnn_encoded_channels: 10  # Compressed obs channels before RNN (8-12 recommended)
  rnn_num_layers: 2  # Number of ConvLSTM layers
  use_rnn_memory: true  # Enable RNN memory encoding

optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

scheduler:
  type: cosine
  warmup_epochs: 5
  min_lr: 1.0e-6

logging:
  experiment_name: behavior_cloning
  save_dir: experiments
  tensorboard: true
  use_wandb: true
  wandb_project: generals-rl
  wandb_tags: [behavior_cloning, base]

seed: 42

