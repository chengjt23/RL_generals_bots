experiment:
  name: "offline_sac_cql_bc"
  bc_pretrain_path: "/root/shared-nvme/cjt/experiments/bc_trainvalue_nstep_td_20251215_174316/checkpoints/epoch_28_loss_-0.8559-1.9595-1.9595.pt"
  save_dir: "experiments"

model:
  grid_size: 24
  obs_channels: 15
  memory_channels: 20
  base_channels: 64

data:
  data_dir: "data/generals_io_replays"
  num_workers: 4
  max_replays: null
  min_stars: 70

training:
  device: "cuda"
  seed: 42
  
  # SAC parameters
  gamma: 0.99
  tau: 0.005
  alpha: 0.4
  auto_tune_alpha: true
  
  # Offline RL parameters (set to 0 to disable)
  cql_alpha: 2.0          # CQL penalty weight (0=disabled, typical: 1.0-5.0)
  bc_weight: 0.05          # BC regularization weight (0=disabled, typical: 0.05-0.5)
  gradient_clip: 1.0      # Gradient clipping for stability
  
  # Training schedule
  epochs: 5
  batch_size: 128
  steps_per_epoch: 500
  
  # Learning rates (lower for offline RL)
  actor_lr: 0.00001       # Reduced from 0.00003
  critic_lr: 0.00003      # Reduced from 0.0001
  alpha_lr: 0.0005
  
  log_frequency: 10
  save_frequency: 1

logging:
  use_wandb: true
  project_name: "generals-offline-rl"
  wandb_entity: null

