# 模型架构详细说明

本文档详细描述了SOTA智能体的完整模型架构，包括输入、网络结构和输出的详细规格，便于绘制架构图。

## 总览

模型采用U-Net主干网络 + 双头输出（Policy Head + Value Head）的架构。整体数据流如下：

```
观测输入(15通道) + 记忆增强(18通道) 
    → U-Net主干网络 
    → 特征提取(64通道) 
    → 分支1: Policy Head (预测动作分布)
    → 分支2: Value Head (预测状态价值)
```

### 详细架构图（ASCII版）

```
                         输入层
        ┌────────────────┴────────────────┐
        │                                 │
   观测 (15通道)                    记忆增强 (18通道)
   [15, 24, 24]                     [18, 24, 24]
        │                                 │
        └────────────────┬────────────────┘
                         │
                   拼接 (Concat)
                   [33, 24, 24]
                         │
        ┌────────────────┴────────────────┐
        │         U-Net 主干网络           │
        │                                 │
        │    编码器 (Encoder Path)        │
        │    ┌─────────────────┐          │
        │    │ Enc1 [64,24,24] │──┐       │
        │    │    ↓ Pool       │  │       │
        │    │ Enc2 [128,12,12]│──┤       │
        │    │    ↓ Pool       │  │       │
        │    │ Enc3 [256,6,6]  │──┤       │
        │    │    ↓ Pool       │  │       │
        │    └─────────────────┘  │       │
        │                         │       │
        │  瓶颈层 [512, 3, 3]     │       │
        │                         │       │
        │    解码器 (Decoder Path) │       │
        │    ┌─────────────────┐  │       │
        │    │    ↑ UpConv     │  │       │
        │    │ Dec3 [256,6,6]  │←─┘ (skip)│
        │    │    ↑ UpConv     │  │       │
        │    │ Dec2 [128,12,12]│←─┘ (skip)│
        │    │    ↑ UpConv     │  │       │
        │    │ Dec1 [64,24,24] │←─┘ (skip)│
        │    └─────────────────┘          │
        │                                 │
        └────────────────┬────────────────┘
                         │
                 特征图 [64, 24, 24]
                         │
        ┌────────────────┴────────────────┐
        │                                 │
    Policy Head                      Value Head
        │                                 │
  ConvBlock (32ch)                  ConvBlock (32ch)
        │                                 │
   Conv 1x1 (9ch)                      Flatten
        │                                 │
  [9, 24, 24]                        FC (256)
        │                                 │
        │                             FC (1)
        │                                 │
        ↓                                 ↓
  动作分布 logits                     状态价值
  (Pass + 8种移动)                      [1]
```

---

## 1. 输入层详细说明

### 1.1 观测输入 (Observation Input)

**维度**: `[batch_size, 15, 24, 24]`

游戏环境提供的原始观测包含**15个通道**，每个通道为24x24的网格：

| 通道索引 | 通道名称 | 说明 |
|---------|---------|------|
| 0 | armies | 每个格子的军队数量 |
| 1 | generals | 将军位置（1表示有将军，0表示无） |
| 2 | cities | 城市位置（1表示有城市，0表示无） |
| 3 | mountains | 山脉位置（1表示有山脉，0表示无） |
| 4 | neutral_cells | 中立格子（未被占领） |
| 5 | owned_cells | 己方占领的格子 |
| 6 | opponent_cells | 对手占领的格子 |
| 7 | fog_cells | 战争迷雾格子（未探索区域） |
| 8 | structures_in_fog | 迷雾中的结构 |
| 9-14 | (其他游戏状态信息) | 额外的游戏状态编码 |

### 1.2 记忆增强 (Memory Augmentation)

**维度**: `[batch_size, 18, 24, 24]`

为了应对部分可观测性，模型引入记忆增强机制，包含**18个通道**：

#### 基础记忆通道 (4个)

| 通道索引 | 通道名称 | 说明 |
|---------|---------|------|
| 0 | discovered_castles | 已发现的城堡位置（持久化） |
| 1 | discovered_generals | 已发现的将军位置（持久化） |
| 2 | explored_cells | 己方已探索的格子（持久化） |
| 3 | opponent_visible_cells | 对手已探索的格子（持久化） |

#### 动作历史通道 (14个 = 7轮 × 2方)

记录最近7轮的己方和对手动作，每轮包含2个通道：

| 通道范围 | 说明 |
|---------|------|
| 4-5 | 第1轮历史（己方动作 + 对手动作） |
| 6-7 | 第2轮历史（己方动作 + 对手动作） |
| 8-9 | 第3轮历史（己方动作 + 对手动作） |
| 10-11 | 第4轮历史（己方动作 + 对手动作） |
| 12-13 | 第5轮历史（己方动作 + 对手动作） |
| 14-15 | 第6轮历史（己方动作 + 对手动作） |
| 16-17 | 第7轮历史（己方动作 + 对手动作） |

**动作编码**：在动作地图中，每个格子的值表示该位置的移动方向+1：
- 0: 无动作
- 1: 向上移动 (direction=0)
- 2: 向下移动 (direction=1)
- 3: 向左移动 (direction=2)
- 4: 向右移动 (direction=3)

### 1.3 输入拼接

最终输入到网络的张量维度为：
```
[batch_size, 33, 24, 24]  (15 + 18 = 33个通道)
```

---

## 2. U-Net主干网络 (Backbone)

U-Net是一个对称的编码器-解码器架构，擅长空间特征提取和保留位置信息。

### 2.1 网络结构概览

```
输入 [33, 24, 24]
    ↓
编码器路径 (Encoder Path)
    ↓
瓶颈层 (Bottleneck)
    ↓
解码器路径 (Decoder Path)
    ↓
输出 [64, 24, 24]
```

### 2.2 详细层级结构

假设 `base_channels = 64`（可配置，也可设为128）

#### 编码器路径 (下采样)

| 层级 | 操作 | 输入维度 | 输出维度 |
|------|------|---------|---------|
| **Encoder 1** | ConvBlock → ConvBlock | [33, 24, 24] | [64, 24, 24] |
| Pool 1 | MaxPool2d(2) | [64, 24, 24] | [64, 12, 12] |
| **Encoder 2** | ConvBlock → ConvBlock | [64, 12, 12] | [128, 12, 12] |
| Pool 2 | MaxPool2d(2) | [128, 12, 12] | [128, 6, 6] |
| **Encoder 3** | ConvBlock → ConvBlock | [128, 6, 6] | [256, 6, 6] |
| Pool 3 | MaxPool2d(2) | [256, 6, 6] | [256, 3, 3] |

#### 瓶颈层

| 层级 | 操作 | 输入维度 | 输出维度 |
|------|------|---------|---------|
| **Bottleneck** | ConvBlock → ConvBlock | [256, 3, 3] | [512, 3, 3] |

#### 解码器路径 (上采样 + 跳跃连接)

| 层级 | 操作 | 输入维度 | 跳跃连接 | 输出维度 |
|------|------|---------|---------|---------|
| UpConv 3 | ConvTranspose2d | [512, 3, 3] | - | [256, 6, 6] |
| Concat 3 | torch.cat | [256, 6, 6] | + Enc3 [256, 6, 6] | [512, 6, 6] |
| **Decoder 3** | ConvBlock → ConvBlock | [512, 6, 6] | - | [256, 6, 6] |
| UpConv 2 | ConvTranspose2d | [256, 6, 6] | - | [128, 12, 12] |
| Concat 2 | torch.cat | [128, 12, 12] | + Enc2 [128, 12, 12] | [256, 12, 12] |
| **Decoder 2** | ConvBlock → ConvBlock | [256, 12, 12] | - | [128, 12, 12] |
| UpConv 1 | ConvTranspose2d | [128, 12, 12] | - | [64, 24, 24] |
| Concat 1 | torch.cat | [64, 24, 24] | + Enc1 [64, 24, 24] | [128, 24, 24] |
| **Decoder 1** | ConvBlock → ConvBlock | [128, 24, 24] | - | [64, 24, 24] |

### 2.3 ConvBlock详细结构

每个ConvBlock包含：
```python
Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
    ↓
BatchNorm2d(out_channels)
    ↓
ReLU激活
```

**参数说明**：
- kernel_size=3：3x3卷积核
- padding=1：保持空间维度不变
- BatchNorm：批归一化，稳定训练
- ReLU：非线性激活函数

---

## 3. Policy Head (策略头)

### 3.1 功能
预测在每个格子采取每种动作的对数概率（logits）

### 3.2 结构

| 层级 | 操作 | 输入维度 | 输出维度 |
|------|------|---------|---------|
| ConvBlock | Conv + BN + ReLU | [64, 24, 24] | [32, 24, 24] |
| Conv2d | 1x1卷积 | [32, 24, 24] | [9, 24, 24] |

### 3.3 输出详解

**输出维度**: `[batch_size, 9, 24, 24]`

**9个通道的含义**：

| 通道索引 | 动作类型 | 说明 |
|---------|---------|------|
| 0 | Pass | 本回合不采取行动 |
| 1 | 向上移动 + 全部军队 | direction=0, split=0 |
| 2 | 向上移动 + 一半军队 | direction=0, split=1 |
| 3 | 向下移动 + 全部军队 | direction=1, split=0 |
| 4 | 向下移动 + 一半军队 | direction=1, split=1 |
| 5 | 向左移动 + 全部军队 | direction=2, split=0 |
| 6 | 向左移动 + 一半军队 | direction=2, split=1 |
| 7 | 向右移动 + 全部军队 | direction=3, split=0 |
| 8 | 向右移动 + 一半军队 | direction=3, split=1 |

**动作采样流程**：
1. 对于通道0（Pass），取位置[0,0]的logit值
2. 对于通道1-8，每个格子[r,c]的值表示在该位置采取对应动作的logit
3. 应用valid_mask过滤掉非法动作
4. 使用softmax转换为概率分布
5. 使用argmax（推理时）或采样（训练时）选择动作

---

## 4. Value Head (价值头)

### 4.1 功能
预测当前游戏状态的价值（赢的概率）

### 4.2 结构

| 层级 | 操作 | 输入维度 | 输出维度 |
|------|------|---------|---------|
| ConvBlock | Conv + BN + ReLU | [64, 24, 24] | [32, 24, 24] |
| Flatten | 展平 | [32, 24, 24] | [18432] |
| Linear | 全连接层 | [18432] | [256] |
| ReLU | 激活 | [256] | [256] |
| Linear | 全连接层 | [256] | [1] |

### 4.3 输出详解

**输出维度**: `[batch_size, 1]`

**输出含义**：
- 单个标量值，表示当前状态的价值估计
- 训练时使用MSE损失与实际收益对齐
- 在强化学习中用于优势估计（GAE）

---

## 5. 完整数据流示例

### 5.1 前向传播示例

假设batch_size=1：

```
步骤1: 准备输入
- 观测张量: [1, 15, 24, 24]
- 记忆张量: [1, 18, 24, 24]
- 拼接: [1, 33, 24, 24]

步骤2: U-Net主干
- 输入: [1, 33, 24, 24]
- Encoder1: [1, 33, 24, 24] → [1, 64, 24, 24]
- Pool1 + Encoder2: [1, 64, 12, 12] → [1, 128, 12, 12]
- Pool2 + Encoder3: [1, 128, 6, 6] → [1, 256, 6, 6]
- Pool3 + Bottleneck: [1, 256, 3, 3] → [1, 512, 3, 3]
- Decoder3 (含skip): [1, 512, 6, 6] → [1, 256, 6, 6]
- Decoder2 (含skip): [1, 256, 12, 12] → [1, 128, 12, 12]
- Decoder1 (含skip): [1, 128, 24, 24] → [1, 64, 24, 24]
- 输出特征: [1, 64, 24, 24]

步骤3: Policy Head
- 输入: [1, 64, 24, 24]
- ConvBlock: [1, 64, 24, 24] → [1, 32, 24, 24]
- Conv1x1: [1, 32, 24, 24] → [1, 9, 24, 24]
- 输出: policy_logits [1, 9, 24, 24]

步骤4: Value Head
- 输入: [1, 64, 24, 24]
- ConvBlock: [1, 64, 24, 24] → [1, 32, 24, 24]
- Flatten: [1, 32, 24, 24] → [1, 18432]
- FC1: [1, 18432] → [1, 256]
- ReLU: [1, 256] → [1, 256]
- FC2: [1, 256] → [1, 1]
- 输出: value [1, 1]
```

### 5.2 训练损失

在行为克隆阶段：
```python
loss = policy_loss  # 交叉熵损失，预测专家动作
```

在强化学习阶段（PPO）：
```python
loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus
```

---

## 6. 模型参数统计

### 6.1 可配置参数

| 参数名称 | 默认值 | 说明 |
|---------|-------|------|
| obs_channels | 15 | 观测通道数 |
| memory_channels | 18 | 记忆通道数（BC阶段可设为0） |
| grid_size | 24 | 网格大小 |
| base_channels | 64 | U-Net基础通道数（可改为128提升性能） |

### 6.2 总参数量估算

以base_channels=64为例：
- U-Net主干: ~2.3M 参数
- Policy Head: ~20K 参数
- Value Head: ~4.7M 参数
- **总计**: 约 **7M 参数**

---

## 7. 关键设计特点

### 7.1 U-Net的优势
- **跳跃连接**: 保留低级空间特征，帮助精确定位
- **对称结构**: 下采样提取全局特征，上采样恢复空间分辨率
- **多尺度特征**: 同时捕获局部战术和全局战略

### 7.2 记忆增强的重要性
- 应对部分可观测性（战争迷雾）
- 记录已发现的关键结构（将军、城市）
- 追踪历史动作，学习序列模式
- 未来可升级为LSTM或Transformer记忆

### 7.3 双头输出
- **Policy Head**: 动作选择
- **Value Head**: 状态评估
- 两者共享U-Net特征，提高样本效率

---

## 8. 训练流程概览

### 8.1 第一阶段：行为克隆 (Behavior Cloning)
```
输入: (观测, 专家动作) 数据对
损失: CrossEntropyLoss(预测动作, 专家动作)
优化: Adam优化器
时长: 3小时 (H100 GPU)
```

### 8.2 第二阶段：自博弈强化学习 (PPO Self-Play)
```
输入: 自博弈对局经验
损失: PPO loss (policy + value + entropy)
对手池: 保留最近3个版本
训练: 36小时 (H100 GPU)
奖励整形: 势函数 (land/army/castle比例)
```

---

## 9. 推理流程

```python
# 1. 准备输入
obs_tensor = prepare_observation(obs)  # [1, 15, 24, 24]
memory_tensor = prepare_memory()       # [1, 18, 24, 24]

# 2. 模型前向传播
policy_logits, value = model(obs_tensor, memory_tensor)
# policy_logits: [1, 9, 24, 24]
# value: [1, 1]

# 3. 应用动作掩码
valid_mask = compute_valid_mask(obs)  # [24, 24, 4]
masked_logits = apply_mask(policy_logits, valid_mask)

# 4. 选择动作
action = argmax(masked_logits)  # 或使用温度采样

# 5. 解码动作
if action_type == "pass":
    return Action(to_pass=True)
else:
    row, col, direction, split = decode_action(action)
    return Action(to_pass=False, row=row, col=col, 
                  direction=direction, to_split=split)

# 6. 更新记忆
memory.update(obs, action, opponent_action)
```

---

## 10. 可视化架构图建议

绘制架构图时，建议包含以下模块：

1. **输入层**
   - 观测输入 (15通道)
   - 记忆输入 (18通道)
   - 拼接操作

2. **U-Net主干**
   - 编码器路径（3层下采样）
   - 瓶颈层
   - 解码器路径（3层上采样 + 跳跃连接）

3. **输出头**
   - Policy Head → [9, 24, 24]
   - Value Head → [1]

4. **数据流箭头**
   - 主路径：实线箭头
   - 跳跃连接：虚线箭头
   - 标注张量维度

5. **颜色编码**
   - 输入层：蓝色
   - 卷积层：绿色
   - 池化/上采样：橙色
   - 输出层：红色

---

## 参考文献

- U-Net: Ronneberger et al., 2015
- PPO: Schulman et al., 2017
- GAE: Schulman et al., 2018
- 势函数奖励整形: Ng et al., 1999
- 本项目参考论文: Perolat et al., 2022

---

**文档版本**: 1.0  
**最后更新**: 2026-01-01  
**维护者**: RL_generals_bots 团队
