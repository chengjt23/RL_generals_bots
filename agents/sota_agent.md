5 Agent  Our agent development consists of two main steps. First, behavior cloning on curated expert replays establishes an initial policy. This policy is then refined via self-play fine-tuning with reinforcement learning, where the agent iteratively improves by competing against a dynamic pool of its prior versions. To navigate the sparse reward landscape during this reinforcement learning phase, potentialbased reward shaping is employed to provide more consistent learning signals and guide towards more robust policies. The specifics of these primary training stages and supporting mechanisms are detailed in the following text.  Behavior Cloning. A corpus of 347 000 raw replays was obtained from the Generals.io AWS bucket. This dataset has many outliers, for example players willingly not finishing their game, being idle, or being played on different (but similar) versions of the game. We observe that long games are strongly associated with atypical, very often suboptimal behavior of players. Therefore, we filter games longer than 500 turns (equal to 1000 moves). We further resort to games played on the newest patch of the game. There is a trade-off between the size of the dataset and the quality of the dataset. We observe that in our particular scenario, agents perform better when trained on high-quality replays of high-rated players (compared to a larger dataset that includes lower-quality games). Therefore, we further restrict ourselves to games that feature at least one participant with a ≥ 70-star rating. This yields a dataset of 16 320 games. We then perform behavior cloning by predicting the next move of human players for 3 hours wall clock time on a single H100 GPU. This behavior cloning agent is already powerful enough to beat mediocre players, but is very narrow in its behavior and can be easily exploited.  Self-Play Fine-Tuning. After behavior cloning, we refine the agent via self-play using Proximal Policy Optimization (Schulman et al., 2017). Because episodes span hundreds of steps and our hardware limits batch sizes, we rely on Generalized Advantage Estimation (Schulman et al., 2018) with λ = 0.95 for stable learning. Each new agent faces its N most recent predecessors, which act via arg max; we observed that training against stochastic opponents makes new agents focus on exploiting mistakes caused by stochasticity, instead of focusing on more robust gameplay. When a trained candidate achieves a 45 % win-rate versus the current pool (approximately 55 % with both sides using arg max), it replaces the oldest model. Our top model was trained for 36 hours on a single H100 GPU with an opponent-pool size of N = 3.  Memory Augmentation. To play effectively under partial observability, an agent must carry forward critical information from earlier in the game. We therefore augment each raw observation with a small “memory stack” of additional feature-maps that encode what the agent has already discovered. Specifically, before feeding data into our network, we augment observations by information that contains (1) the positions of any castles or generals that have been revealed, (2) which grid cells the agent itself has already explored (so it does not waste time re-searching the same ground), (3) which cells the agent knows its opponent has seen (which both signals where the opponent has scouted and indirectly marks territory behind the fog-of-war that the enemy owns), and (4) the last seven moves taken by each side.  However, this hand-crafted memory augmentation does not capture all the details that an agent should remember. For example, the agent does not know when it uncovered certain parts of the map, or what were the exact army counts of these cells at that time. We anticipate that incorporating a recurrent architecture or transformer-style memory would allow the network to aggregate and remember those counts and timings, as well as understand that this information is not completely reliable. We leave that extension to future work.  Architecture. Our policy is parametrized by a convolutional neural network. We use the same architecture as Perolat et al. (2022), where an U-Net torso processes the augmented observation to produce a board game embedding. This representation is then provided to the value and policy heads Pyramid Backbone  Observation Memory Augmentation Policy Prediction  Value Prediction  Figure 3: The environment produces game observation encoded as a 3D tensor. This observation is further enriched by memory features and fed into a U-Net feature extractor. The resulting game representation is then passed to value and policy heads, predicting value and action distributions.  (see Figure 3). The policy head returns an H ×W ×9 tensor that encodes a distribution over actions. The nine numbers encode whether the agent wants to pass or move all or half of his units in one of the four directions for each cell.  Reward Shaping. Rewards in games are often very sparse, usually just 1 (win) and -1 (lose). Due to long episodes and computational constraints, we provide an additional training signal through potential-based reward shaping (Ng et al., 1999). This type of shaping uses a so-called potential function on states φ : S → R and the shaped reward for performing an action a and moving from state s ∈ S to s′ ∈ S is expressed as  rshaped(s, a, s′) = roriginal(s, a, s′) + γφ(s′) − φ(s),  where roriginal is a simple win/lose reward and γ ∈ (0, 1] is the discount factor. This type of shaping has a property that the set of optimal policies does not change when moving from roriginal to rshaped. We define the potential function φ as the following weighted sum of three log-ratio features:  φ(s) = 0.3 φland(s) + 0.3 φarmy(s) + 0.4 φcastle(s),  where each sub-potential is  φx(s) = log xagent(s)/xenemy(s)  log(max_ratio) for x ∈ {land, army, castle}.  Here max_ratio is the normalization constant used to bound each log-ratio in [−1, 1]. The logarithm makes the reward symmetric around a ratio of 1. Consequently, φ(s) is larger whenever our land, army, or castle counts exceed the opponent’s, guiding the agent to states of material advantage. We observe that without reward shaping, the agent quickly converges to an aggressive behavior, where most of the time it focuses entirely on gathering army and attacking, which can be fended off by experienced players who defend well. With reward shaping, the agent’s focus shifts toward building material advantage and avoiding unnecessary risks, making it much more robust in all stages of the game.